import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.cluster import KMeans
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix


# ðŸ”¹ Change the filename here for other datasets
data = pd.read_csv("Titanic-Dataset.csv")

print("âœ… Dataset Loaded Successfully")
print("Shape:", data.shape)
data.head()


# Fill missing numeric columns with mean
for col in data.select_dtypes(include=[np.number]):
    data[col].fillna(data[col].mean(), inplace=True)

# Fill missing categorical columns with mode
for col in data.select_dtypes(include=['object']):
    data[col].fillna(data[col].mode()[0], inplace=True)

# Encode categorical columns
le = LabelEncoder()
for col in data.select_dtypes(include=['object']):
    data[col] = le.fit_transform(data[col])

print("âœ… Missing values handled and categorical columns encoded.")
data.head()



# ðŸ”¹ Change 'Survived' below to your datasetâ€™s target column name if different
y = data['Survived']
X = data.drop('Survived', axis=1)

# Split train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
sc = StandardScaler()
X_train_scaled = sc.fit_transform(X_train)
X_test_scaled = sc.transform(X_test)

print("âœ… Data split and scaled.")




# Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_train_scaled, y_train)
y_pred_lr = lr.predict(X_test_scaled)
acc_lr = accuracy_score(y_test, y_pred_lr)

# Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
acc_rf = accuracy_score(y_test, y_pred_rf)

print(f"ðŸ”¹ Logistic Regression Accuracy: {acc_lr:.2f}")
print(f"ðŸ”¹ Random Forest Accuracy: {acc_rf:.2f}")




print("\nRandom Forest Report:\n", classification_report(y_test, y_pred_rf))

sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - Random Forest")
plt.show()






# ðŸ”¹ Choose number of clusters (change n_clusters for different datasets)
kmeans = KMeans(n_clusters=3, random_state=20)
clusters = kmeans.fit_predict(X)

data['Cluster'] = clusters

plt.scatter(data.iloc[:, 0], data.iloc[:, 1], c=clusters, cmap='viridis')
plt.title("K-Means Clustering")
plt.xlabel(data.columns[0])
plt.ylabel(data.columns[1])
plt.show()




data.hist(figsize=(10, 8), bins=20)
plt.suptitle("Histograms of Features")
plt.show()

# Bar plot of average feature values per cluster
data.groupby('Cluster').mean().plot(kind='bar', figsize=(10, 6))
plt.title("Feature Means by Cluster")
plt.show()

# Line plot for first few features
data.iloc[:30, :5].plot(kind='line', figsize=(10, 5))
plt.title("Line Plot of First 5 Features (Sample 30 Rows)")
plt.show()




sample = X_test.iloc[0].values.reshape(1, -1)
print("Predicted class (Random Forest):", rf.predict(sample)[0])
print("Predicted class (Logistic Regression):", lr.predict(sc.transform(sample))[0])